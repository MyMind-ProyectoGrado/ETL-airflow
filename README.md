# MyMind ETL Pipeline

Un pipeline ETL robusto y escalable construido con Apache Airflow que extrae datos de MongoDB y los carga en un data warehouse MySQL con replicaci√≥n master-slave para alta disponibilidad.

## üèóÔ∏è Arquitectura

```
MongoDB (MyMind) ‚Üí Apache Airflow ‚Üí MySQL Master ‚Üí MySQL Slave
                                      ‚Üì
                                  Data Warehouse
```

### Componentes Principales

- **Apache Airflow**: Orquestaci√≥n y programaci√≥n de tareas ETL
- **MySQL Master-Slave**: Replicaci√≥n para alta disponibilidad
- **MongoDB**: Base de datos fuente (MyMind users y transcriptions)
- **Docker Compose**: Containerizaci√≥n y gesti√≥n de servicios

## üìã Caracter√≠sticas

- ‚úÖ ETL automatizado cada 6 horas (configurable)
- ‚úÖ Replicaci√≥n MySQL master-slave autom√°tica
- ‚úÖ Failover autom√°tico en caso de fallo del master
- ‚úÖ Sincronizaci√≥n bidireccional de datos
- ‚úÖ Monitoreo y logging detallado
- ‚úÖ Manejo de tipos BSON (ObjectId, DateTime)
- ‚úÖ Vista `study_data` para an√°lisis de investigaci√≥n
- ‚úÖ Pruebas de integraci√≥n automatizadas

## üöÄ Inicio R√°pido

### Prerrequisitos

- Docker y Docker Compose
- Python 3.8+
- Acceso a MongoDB (MyMind)

### Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd airflow-etl-pipeline
```

2. **Configurar variables de entorno**
```bash
# Crear archivo .env
echo "AIRFLOW_UID=$(id -u)" > .env
echo "MYSQL_ROOT_PASSWORD=rootpassword" >> .env
echo "MYSQL_DATABASE=mymind_dw" >> .env
echo "MYSQL_USER=airflow_user" >> .env
echo "MYSQL_PASSWORD=airflow_pass" >> .env
```

3. **Levantar los servicios**
```bash
docker-compose up -d
```

4. **Configurar replicaci√≥n MySQL**
```bash
./mysql/setup-replication.sh
```

5. **Acceder a Airflow UI**
- URL: http://localhost:8080
- Usuario: `airflow`
- Contrase√±a: `airflow`

## üîß Configuraci√≥n

### Conexiones de Airflow

Una vez en la UI de Airflow, configurar las siguientes conexiones:

#### Conexi√≥n MongoDB (`mongo_mymind`)
- **Conn Type**: Generic
- **Host**: `<mongodb-host>`
- **Login**: `<mongodb-user>`
- **Password**: `<mongodb-password>`
- **Extra**: 
```json
{
  "srv": true,
  "connectTimeoutMS": 10000,
  "serverSelectionTimeoutMS": 30000
}
```

#### Conexi√≥n MySQL (`mysql_mymind_dw`)
- **Conn Type**: MySQL
- **Host**: `mysql_mymind_master`
- **Login**: `airflow_user`
- **Password**: `airflow_pass`
- **Schema**: `mymind_dw`
- **Port**: `3306`

### Programaci√≥n del DAG

El DAG se ejecuta autom√°ticamente cada 6 horas. Para modificar la frecuencia:

```python
# En dags/mymind_etl_dag.py
schedule="0 0,6,12,18 * * *"  # Cada 6 horas
# schedule="0 * * * *"         # Cada hora
# schedule="*/30 * * * *"      # Cada 30 minutos
```

## üìä Estructura de Datos

### Tabla `users`
- `user_id` (VARCHAR, PK)
- `name`, `email`, `profile_pic`
- `birthdate`, `city`, `personality`
- `university`, `degree`, `gender`
- `notifications`, `accept_policies`
- `acceptance_date`, `acceptance_ip`
- `allow_anonimized_usage`

### Tabla `transcriptions`
- `transcription_id` (VARCHAR, PK)
- `user_id` (FK a users)
- `transcription_date`, `transcription_time`
- `text`, `emotion`, `sentiment`, `topic`
- Probabilidades de emociones: `emotion_probs_*`
- Probabilidades de sentimiento: `sentiment_probs_*`

### Vista `study_data`
Vista optimizada para an√°lisis de investigaci√≥n que incluye:
- Transcripciones con an√°lisis emocional
- Datos demogr√°ficos anonimizados
- Solo usuarios que aceptaron pol√≠ticas

## üîÑ Alta Disponibilidad

### Replicaci√≥n Master-Slave

El sistema incluye replicaci√≥n autom√°tica:

```bash
# Monitorear estado de replicaci√≥n
./mysql/monitor.sh

# Sincronizaci√≥n manual completa
./mysql/sync-databases.sh
```

### Failover Autom√°tico

En caso de fallo del master, el sistema:

1. Detecta la ca√≠da del master
2. Promueve el slave a master
3. Actualiza autom√°ticamente las conexiones de Airflow
4. Env√≠a notificaciones al administrador

```bash
# Ejecutar failover manual
./mysql/failover.sh
```

## üß™ Pruebas

### Pruebas de Integraci√≥n

```bash
# Ejecutar todas las pruebas
python integration_tests/test_failover.py

# Revisar logs de pruebas
tail -f failover_tests.log
```

Las pruebas incluyen:
- IT-07-01: Verificaci√≥n de replicaci√≥n master-slave
- IT-07-02: Simulaci√≥n de fallo y failover
- IT-07-03: Continuidad de operaciones post-failover
- IT-08-01: Ejecuci√≥n ETL despu√©s del failover
- IT-08-02: Actualizaci√≥n de conexiones Airflow

## üìà Monitoreo

### Servicios Disponibles

- **Airflow UI**: http://localhost:8080
- **MySQL Master**: localhost:3307
- **MySQL Slave**: localhost:3308
- **Flower (Celery)**: http://localhost:5555 (con `--profile flower`)

### Logs Importantes

```bash
# Logs de Airflow
docker-compose logs airflow-scheduler
docker-compose logs airflow-worker

# Logs de MySQL
docker-compose logs mysql_mymind
docker-compose logs mysql_mymind_slave

# Logs del DAG ETL
# Disponibles en Airflow UI > DAGs > mymind_mongo_to_mysql_etl > Logs
```

## üõ†Ô∏è Mantenimiento

### Comandos √ötiles

```bash
# Reiniciar todos los servicios
docker-compose restart

# Ver estado de contenedores
docker-compose ps

# Limpiar vol√∫menes (¬°CUIDADO: Elimina datos!)
docker-compose down -v

# Acceder a MySQL master
docker exec -it mysql_mymind_master mysql -uroot -p

# Ejecutar DAG manualmente
docker exec -it <airflow-container> airflow dags trigger mymind_mongo_to_mysql_etl
```

### Backup de Datos

```bash
# Backup completo de MySQL
docker exec mysql_mymind_master mysqldump -uroot -prootpassword --all-databases > backup_$(date +%Y%m%d).sql

# Restaurar desde backup
cat backup_20240501.sql | docker exec -i mysql_mymind_master mysql -uroot -prootpassword
```

## üîç Troubleshooting

### Problemas Comunes

**Error de conexi√≥n a MongoDB**
```bash
# Verificar conectividad
docker exec airflow-airflow-worker-1 python -c "from pymongo import MongoClient; print(MongoClient('mongodb://...').admin.command('ping'))"
```

**Replicaci√≥n MySQL no funciona**
```bash
# Verificar estado de replicaci√≥n
./mysql/monitor.sh

# Reconfigurar replicaci√≥n
./mysql/setup-replication.sh
```

**DAG no se ejecuta**
```bash
# Verificar que el DAG est√© activado en la UI
# Revisar logs del scheduler
docker-compose logs airflow-scheduler
```

### Contacto y Soporte

Para problemas o preguntas:
- Revisar logs detallados en Airflow UI
- Consultar documentaci√≥n de troubleshooting
- Verificar configuraci√≥n de conexiones

## üìÑ Licencia

Este proyecto est√° bajo la Licencia Apache 2.0. Ver archivo `LICENSE` para m√°s detalles.

---

**Nota**: Este sistema maneja datos sensibles de usuarios. Aseg√∫rate de cumplir con las regulaciones de privacidad y protecci√≥n de datos aplicables (GDPR, CCPA, etc.).
